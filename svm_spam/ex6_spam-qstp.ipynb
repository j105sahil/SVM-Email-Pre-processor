{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6-2: Spam Classification with SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, we will build a spam filter with SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Email Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process_email()` preprocesses a the body of an email and returns a list of word indices with following preprocessing and normalization steps:\n",
    "\n",
    "- Lower-casing\n",
    "- Stripping HTML\n",
    "- Normalizing URLs\n",
    "- Normalizing Email Addresses\n",
    "- Normalizing Numbers\n",
    "- Normalizing Dollars\n",
    "- Word Stemming\n",
    "- Removal of non-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\n",
    "def process_email(email_contents):\n",
    "    \"\"\"\n",
    "    Preprocesses a the body of an email and returns a list of word_indices\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    email_contents : string\n",
    "        The email content.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of word indices.\n",
    "\n",
    "    \"\"\"\n",
    "    vocab_list = get_vocablist()\n",
    "    \"\"\"\n",
    "    <[^<>]+>\n",
    "    [0-9]+\n",
    "    (http|https)://[^\\s]*\n",
    "    [^\\s]+@[^\\s]+\n",
    "    [$]+\n",
    "    Remove this from email. use re.sub()\n",
    "    \"\"\"\n",
    "    email_contents = email_contents.lower()\n",
    "    email_contents = re.sub(\"<[^<>]+>\",' ',email_contents)\n",
    "    email_contents = re.sub(\"[0-9]+\",'number',email_contents)\n",
    "    email_contents = re.sub(\"(http|https)://[^\\s]*\",\"httpaddr\",email_contents)\n",
    "    email_contents = re.sub(\"[^\\s]+@[^\\s]+\",\"emailaddr\",email_contents)\n",
    "    email_contents = re.sub(\"[$]+\",'dollar',email_contents)\n",
    "    \"\"\"\n",
    "    use split() for \"\" @$/#.-:&*+=[]?!(){},'\">_<;%\\n\\r\"\"\n",
    "    learn stemming\n",
    "    \"\"\"\n",
    "    word_indices = []\n",
    "    words_list = split(\"\"\" @$/#.-:&*+=[]?!(){},'\">_<;%\\n\\r\"\"\",email_contents)\n",
    "    stemmer = PorterStemmer()\n",
    "    for word in words_list:\n",
    "        if word =='':\n",
    "            continue\n",
    "        word = stemmer.stem(word)\n",
    "        print(word)\n",
    "        for i in range(len(vocab_list)):\n",
    "            if vocab_list[i]==word:\n",
    "                word_indices.append(i)\n",
    "            \n",
    "    \n",
    "    return word_indices\n",
    "\n",
    "\n",
    "def split(delimiters, string, maxsplit=0):\n",
    "    pattern = '|'.join(map(re.escape, delimiters))\n",
    "    return re.split(pattern, string, maxsplit)\n",
    "\n",
    "\n",
    "def get_vocablist():\n",
    "    \"\"\"\n",
    "    Reads the fixed vocabulary list in vocab.txt and returns a list of the words.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        The vocabulary list.\n",
    "    \"\"\"\n",
    "    vocabulary = []\n",
    "    with open('vocab.txt') as f:\n",
    "        for line in f:\n",
    "            idx, word = line.split('\\t')\n",
    "            vocabulary.append(word.strip())\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the raw email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anyon\n",
      "know\n",
      "how\n",
      "much\n",
      "it\n",
      "cost\n",
      "to\n",
      "host\n",
      "a\n",
      "web\n",
      "portal\n",
      "well\n",
      "it\n",
      "depend\n",
      "on\n",
      "how\n",
      "mani\n",
      "visitor\n",
      "you\n",
      "re\n",
      "expect\n",
      "thi\n",
      "can\n",
      "be\n",
      "anywher\n",
      "from\n",
      "less\n",
      "than\n",
      "number\n",
      "buck\n",
      "a\n",
      "month\n",
      "to\n",
      "a\n",
      "coupl\n",
      "of\n",
      "dollarnumb\n",
      "you\n",
      "should\n",
      "checkout\n",
      "httpaddr\n",
      "or\n",
      "perhap\n",
      "amazon\n",
      "ecnumb\n",
      "if\n",
      "your\n",
      "run\n",
      "someth\n",
      "big\n",
      "to\n",
      "unsubscrib\n",
      "yourself\n",
      "from\n",
      "thi\n",
      "mail\n",
      "list\n",
      "send\n",
      "an\n",
      "email\n",
      "emailaddr\n",
      "Word Indices: [85, 915, 793, 1076, 882, 369, 1698, 789, 1821, 1830, 882, 430, 1170, 793, 1001, 1892, 1363, 591, 1675, 237, 161, 88, 687, 944, 1662, 1119, 1061, 1698, 374, 1161, 478, 1892, 1509, 798, 1181, 1236, 809, 1894, 1439, 1546, 180, 1698, 1757, 1895, 687, 1675, 991, 960, 1476, 70, 529, 530]\n"
     ]
    }
   ],
   "source": [
    "with open('emailSample1.txt') as f:\n",
    "    file_contents = f.read().replace('\\n', '')\n",
    "\n",
    "word_indices = process_email(file_contents)\n",
    "\n",
    "# Print Stats\n",
    "print ('Word Indices:', word_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `email_features()` produces a feature vector x from the given word indices. x[i] is 1 if the i-th word is in the email and x[i] is 0 if the i-th word is not present in the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def email_features(word_indices):\n",
    "    \"\"\"\n",
    "    Takes in a word_indices vector and produces a feature vector from the word indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word_indices : array-like\n",
    "        List of word indices.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Feature vector from word indices.\n",
    "    \"\"\"\n",
    "    # Total number of words in the dictionary\n",
    "    n = 1899\n",
    "\n",
    "    x = np.zeros((n, 1))\n",
    "    x[word_indices] = 1\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features from sample email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector: 1899\n",
      "Number of non-zero entries: 45\n"
     ]
    }
   ],
   "source": [
    "features = email_features(word_indices)\n",
    "print ('Length of feature vector:', len(features))\n",
    "print ('Number of non-zero entries:', np.sum(features > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train Linear SVM for Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train SVM to classify spam emails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.97500000000001\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "data = sio.loadmat(\"spamTrain.mat\")\n",
    "\n",
    "# load data from spamTrain.mat\n",
    "X = data[\"X\"] \n",
    "y = data[\"y\"].ravel()\n",
    "\n",
    "C = 0.1\n",
    "#Use Linear SVC\n",
    "clf = svm.LinearSVC(C=C)\n",
    "clf.fit(X,y)\n",
    "p = clf.predict(X)\n",
    "\n",
    "print ('Training Accuracy:', np.mean(p == y) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Test Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained Linear SVM on a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the trained Linear SVM on a test set...\n",
      "Test Accuracy: 99.2\n"
     ]
    }
   ],
   "source": [
    "# load spamTest.mat \n",
    "data = sio.loadmat(\"spamTest.mat\")\n",
    "X_test = data['Xtest']\n",
    "y_test = data['ytest'].ravel()\n",
    "\n",
    "print ('Evaluating the trained Linear SVM on a test set...')\n",
    "p = clf.predict(X_test)\n",
    "\n",
    "print ('Test Accuracy:', np.mean(p == y_test) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Top Predictors of Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictors of spam:\n",
      "our             (0.421665)\n",
      "remov           (0.387173)\n",
      "click           (0.387060)\n",
      "basenumb        (0.346617)\n",
      "guarante        (0.341686)\n",
      "visit           (0.303028)\n",
      "bodi            (0.263523)\n",
      "will            (0.244394)\n",
      "numberb         (0.238795)\n",
      "price           (0.234199)\n",
      "dollar          (0.232315)\n",
      "nbsp            (0.227081)\n",
      "below           (0.223199)\n",
      "lo              (0.219994)\n",
      "most            (0.214548)\n"
     ]
    }
   ],
   "source": [
    "# use coef_.ravel()\n",
    "coef = clf.coef_.ravel()\n",
    "idx = coef.argsort()[::-1]\n",
    "vocab_list = get_vocablist()\n",
    "\n",
    "print ('Top predictors of spam:')\n",
    "for i in range(15):\n",
    "    print (\"{0:<15s} ({1:f})\".format(vocab_list[idx[i]], coef[idx[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Try Your Own Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the spam classifier over `spamSample1.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do\n",
      "you\n",
      "want\n",
      "to\n",
      "make\n",
      "dollarnumb\n",
      "or\n",
      "more\n",
      "per\n",
      "week\n",
      "if\n",
      "you\n",
      "are\n",
      "a\n",
      "motiv\n",
      "and\n",
      "qualifi\n",
      "individu\n",
      "i\n",
      "will\n",
      "person\n",
      "demonstr\n",
      "to\n",
      "you\n",
      "a\n",
      "system\n",
      "that\n",
      "will\n",
      "make\n",
      "you\n",
      "dollarnumb\n",
      "number\n",
      "per\n",
      "week\n",
      "or\n",
      "more\n",
      "thi\n",
      "is\n",
      "not\n",
      "mlm\n",
      "call\n",
      "our\n",
      "number\n",
      "hour\n",
      "pre\n",
      "record\n",
      "number\n",
      "to\n",
      "get\n",
      "the\n",
      "detail\n",
      "number\n",
      "number\n",
      "number\n",
      "i\n",
      "need\n",
      "peopl\n",
      "who\n",
      "want\n",
      "to\n",
      "make\n",
      "seriou\n",
      "money\n",
      "make\n",
      "the\n",
      "call\n",
      "and\n",
      "get\n",
      "the\n",
      "fact\n",
      "invest\n",
      "number\n",
      "minut\n",
      "in\n",
      "yourself\n",
      "now\n",
      "number\n",
      "number\n",
      "number\n",
      "look\n",
      "forward\n",
      "to\n",
      "your\n",
      "call\n",
      "and\n",
      "i\n",
      "will\n",
      "introduc\n",
      "you\n",
      "to\n",
      "peopl\n",
      "like\n",
      "yourself\n",
      "whoar\n",
      "current\n",
      "make\n",
      "dollarnumb\n",
      "number\n",
      "plu\n",
      "per\n",
      "week\n",
      "number\n",
      "number\n",
      "numberljgvnumb\n",
      "numberleannumberlrmsnumb\n",
      "numberwxhonumberqiytnumb\n",
      "numberrjuvnumberhqcfnumb\n",
      "numbereidbnumberdmtvlnumb\n",
      "Processed spamSample1.txt \n",
      "Spam Classification: [1]\n",
      "(1 indicates spam, 0 indicates not spam)\n"
     ]
    }
   ],
   "source": [
    "# load spamSample1.txt\n",
    "#replace \\n by ''\n",
    "filename = 'spamSample1.txt'\n",
    "file  = open(\"spamSample1.txt\",\"r\") \n",
    "\n",
    "file_contents=file.read()\n",
    "file_contents = file_contents.replace(\"\\n\",\"\")\n",
    "word_indices = process_email(file_contents)\n",
    "x = email_features(word_indices)\n",
    "#predict\n",
    "p = clf.predict(x.T)\n",
    "print ('Processed', filename, '\\nSpam Classification:', p)\n",
    "print ('(1 indicates spam, 0 indicates not spam)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
